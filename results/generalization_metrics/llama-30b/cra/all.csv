source,target,target_reference,generalization_accuracy,baseline_accuracy,baseline_intervention,target_tuned_capability,target_tuned_capability_intervention,source_ID_accuracy,RMS_calibration_error,differential_elicitation
alpaca_easy,alpaca_hard,alpaca_hard,0.8933333333333333,0.6506666666666666,zero_shot,0.9733333333333334,classify_lora,0.9346666666666666,0.3024057960134599,0.24931506849315072
arc_easy,arc_hard,arc_hard,0.784,0.608,zero_shot,0.8826666666666667,classify_lora,0.9053333333333333,0.31694942809584326,0.19939577039274928
math_easy,math_hard,math_hard,0.8426666666666667,0.828,zero_shot,0.884,classify_lora,0.9746666666666667,0.31698565311913623,0.016591251885369588
code_easy,code_hard,code_hard,0.7586666666666667,0.76,zero_shot,0.9293333333333333,classify_lora,0.904,0.2728741808305065,-0.0014347202295551982
ranking_logic_easy,ranking_logic_hard,ranking_logic_hard,0.62,0.6546666666666666,zero_shot,0.9933333333333333,classify_lora,0.748,0.2803850226026723,-0.03489932885906036
raven_easy,raven_matrices,raven_matrices,0.5373333333333333,0.7613333333333333,zero_shot,0.948,classify_lora,0.8106666666666666,0.12195418266482407,-0.23628691983122363
alpaca_mmlu,spanish_input,spanish_input,0.768,0.692,zero_shot,0.848,classify_lora,0.8213333333333334,0.2794864066670608,0.08962264150943404
alpaca_mmlu,spanish_output,spanish_output,0.7426666666666667,0.6746666666666666,zero_shot,0.836,classify_lora,0.8213333333333334,0.2757270675570401,0.08133971291866036
alpaca_mmlu,comma_separated_input,comma_separated_input,0.7466666666666667,0.676,zero_shot,0.8533333333333334,classify_lora,0.8213333333333334,0.277709060535405,0.08281249999999998
alpaca_mmlu,comma_separated_output,comma_separated_output,0.764,0.7,zero_shot,0.8786666666666667,classify_lora,0.8213333333333334,0.27548648276014265,0.07283763277693481
alpaca_mmlu,ranking_logic,ranking_logic,0.536,0.6666666666666666,zero_shot,0.9946666666666667,classify_lora,0.8213333333333334,0.2662524759090085,-0.131367292225201
alpaca_mmlu,raven_matrices,raven_matrices,0.6933333333333334,0.7613333333333333,zero_shot,0.948,classify_lora,0.8213333333333334,0.2724092501752815,-0.07172995780590713
alpaca_mmlu,word_swap,word_swap,0.86,0.7746666666666666,zero_shot,0.976,classify_lora,0.8213333333333334,0.30173006034617267,0.08743169398907108
code,counterfactual_python,counterfactual_python,0.756,0.756,zero_shot,0.884,classify_lora,0.8533333333333334,0.30141389487139975,0.0
code,us_history,us_history,0.96,0.668,zero_shot,0.9706666666666667,classify_lora,0.8533333333333334,0.3649393036075801,0.30082417582417575
code,change_my_view,change_my_view,0.604,0.352,zero_shot,0.764,prompt_tuning,0.8533333333333334,0.2757043442263445,0.3298429319371728
cooking,math,math,0.648,0.8626666666666667,zero_shot,0.9186666666666666,classify_lora,0.9293333333333333,0.2940209678506484,-0.23367198838896955
cooking,raven_matrices,raven_matrices,0.7093333333333334,0.7613333333333333,zero_shot,0.948,classify_lora,0.9293333333333333,0.29844886707994595,-0.05485232067510542
math,change_my_view,change_my_view,0.42,0.352,zero_shot,0.764,prompt_tuning,0.8733333333333333,0.28773131155953874,0.08900523560209425
math,cooking,cooking,0.8253333333333334,0.9306666666666666,zero_shot,0.9853333333333333,classify_lora,0.8733333333333333,0.2897380793146356,-0.1069012178619756
change_my_view,raven_matrices,raven_matrices,0.428,0.7613333333333333,zero_shot,0.948,classify_lora,0.744,0.10796245540068285,-0.35161744022503516
change_my_view,cooking,cooking,0.5826666666666667,0.9306666666666666,zero_shot,0.9853333333333333,classify_lora,0.744,0.11370371379414697,-0.3531799729364005
raven_matrices,us_history,us_history,0.9493333333333334,0.668,zero_shot,0.9706666666666667,classify_lora,0.716,0.2690962944189168,0.2898351648351648
raven_matrices,code,code,0.8013333333333333,0.908,zero_shot,0.9626666666666667,classify_lora,0.716,0.25470524961439744,-0.11080332409972302
us_history,math,math,0.62,0.8626666666666667,zero_shot,0.9186666666666666,classify_lora,0.9533333333333334,0.2701242731453551,-0.26415094339622647
us_history,code,code,0.8146666666666667,0.908,zero_shot,0.9626666666666667,classify_lora,0.9533333333333334,0.2922207300189248,-0.09695290858725766
us_history,us_history_textbook,us_history_textbook,0.9426666666666667,0.9786666666666667,zero_shot,0.9906666666666667,classify_lora,0.9533333333333334,0.3173413101037955,-0.036339165545087516
us_history_textbook,us_history_fiction,us_history_fiction,0.9853333333333333,0.7146666666666667,zero_shot,0.9973333333333333,classify_lora,0.98,0.4841938870434926,0.2713903743315508
us_history_fiction,us_history_make_questions,us_history_make_questions,0.9906666666666667,0.37066666666666664,zero_shot,1.0,mms,0.9906666666666667,0.4900750136466507,0.6200000000000001
us_history_make_questions,us_history,us_history,0.9466666666666667,0.668,zero_shot,0.9706666666666667,classify_lora,0.9946666666666667,0.49609774576295484,0.287087912087912
math,math_fiction,math_fiction,0.9,0.936,zero_shot,0.9613333333333334,classify_lora,0.8733333333333333,0.2922724570184342,-0.037447988904299616
math_fiction,math_textbook,math_textbook,0.7893333333333333,0.9333333333333333,zero_shot,0.972,classify_lora,0.8933333333333333,0.30472122087945813,-0.14814814814814817
math_textbook,math_make_questions,math_make_questions,0.8466666666666667,0.9426666666666667,zero_shot,0.9626666666666667,mms,0.7933333333333333,0.29546008400024365,-0.09972299168975067
math_make_questions,math,math,0.716,0.8626666666666667,zero_shot,0.9186666666666666,classify_lora,0.88,0.28257000182780784,-0.15965166908563141
alpaca_low_quality,alpaca_high_quality,alpaca_high_quality,0.49466666666666664,0.12,zero_shot,0.9973333333333333,prompt_tuning,0.924,0.3125954697801937,0.37566844919786097
shp_low_quality,shp_high_quality,shp_high_quality,0.588,0.448,zero_shot,0.628,classify_lora,0.652,0.26674862436137114,0.2229299363057324
code_low_quality,code,code,0.772,0.9093333333333333,zero_shot,0.9626666666666667,classify_lora,0.994,0.3057146181815999,-0.14265927977839332
alpaca_mmlu,truthful_qa,truthful_qa,0.4826666666666667,0.4013333333333333,zero_shot,0.9386666666666666,classify_lora,0.8213333333333334,0.2742833526763498,0.0866477272727273
alpaca_mmlu,personality_traits,personality_traits,0.746,0.888,zero_shot,1.0,classify_lora,0.8213333333333334,0.11514008932580627,-0.14200000000000002
alpaca_mmlu,survival_influence,survival_influence,0.36,0.336,zero_shot,1.0,mms,0.8213333333333334,0.2686556015041288,0.023999999999999966
alpaca_mmlu,gender_bias,gender_bias,0.952,0.086,zero_shot,1.0,mms,0.8213333333333334,0.2851855538593854,0.866
alpaca_mmlu,punishment_avoidance,punishment_avoidance,0.5293333333333333,0.5293333333333333,zero_shot,0.996,classify_lora,0.8213333333333334,0.2608937108079508,0.0
alpaca_mmlu,reward_seeking,reward_seeking,0.49866666666666665,0.48,zero_shot,0.9186666666666666,classify_lora,0.8213333333333334,0.26454072589646765,0.020319303338171262
alpaca_mmlu,crt_1,crt_1,0.64,0.336,zero_shot,1.0,classify_lora,0.8213333333333334,0.2817090482714683,0.304
alpaca_mmlu,crt_2,crt_2,0.136,0.156,zero_shot,0.992,mms,0.8213333333333334,0.2806583269210747,-0.020161290322580634
alpaca_mmlu,crt_3,crt_3,0.676,0.248,zero_shot,1.0,classify_lora,0.8213333333333334,0.2803663661223862,0.42800000000000005
alpaca_mmlu,sycophancy_mimicry,quote_attribution,0.28,0.5,zero_shot,0.968,classify_lora,0.8213333333333334,0.25803505993953846,-0.22727272727272727
alpaca_mmlu,sycophancy_answer,arc_easy,0.356,0.012,zero_shot,0.9533333333333334,classify_lora,0.8213333333333334,0.2906599330822993,0.3608391608391608
alpaca_mmlu,sycophancy_feedback,code_is_correct,0.436,0.492,zero_shot,0.764,classify_lora,0.8213333333333334,0.19877504029046392,-0.07329842931937172
alpaca_chat,sycophancy_are_you_sure,arc_easy,0.34,1.0,zero_shot,1.0,few_shot,0.8426666666666667,0.15227445636728681,-0.6599999999999999
pursue_goals,relinquish_power,relinquish_power,0.992,0.132,zero_shot,1.0,mms,1.0,0.5008285665141483,0.86
creative_writing,biology_with_literary_style,biology_with_literary_style,0.8106666666666666,0.9546666666666667,zero_shot,1.0,mms,0.944,0.31600569794695865,-0.14400000000000002
alpaca_short,alpaca_long,alpaca_mmlu,0.44,0.044,zero_shot,0.8826666666666667,classify_lora,0.82,0.2906301301019612,0.4486404833836858
alpaca_chat,illegal_dont_help,illegal_dont_help,0.4573170731707317,0.5630081300813008,zero_shot,1.0,mms,0.8426666666666667,0.2057807528201278,-0.10569105691056907
alpaca_mmlu,wrong_arc,wrong_arc,0.116,0.16,zero_shot,0.952,classify_lora,0.8213333333333334,0.33766974872677075,-0.046218487394957986
alpaca_mmlu,unhelpful_alpaca,unhelpful_alpaca,0.592,0.14,zero_shot,1.0,mms,0.8213333333333334,0.25084945256006963,0.45199999999999996
alpaca_mmlu,truthful_qa,truthful_qa,0.4826666666666667,0.4013333333333333,zero_shot,0.9386666666666666,classify_lora,0.8213333333333334,0.2742833526763498,0.0866477272727273
alpaca_mmlu,personality_traits,personality_traits,0.746,0.888,zero_shot,1.0,classify_lora,0.8213333333333334,0.11514008932580627,-0.14200000000000002
alpaca_mmlu,gender_bias,gender_bias,0.952,0.086,zero_shot,1.0,mms,0.8213333333333334,0.2851855538593854,0.866
alpaca_mmlu,survival_influence,survival_influence,0.36,0.336,zero_shot,1.0,mms,0.8213333333333334,0.2686556015041288,0.023999999999999966
alpaca_mmlu,punishment_avoidance,punishment_avoidance,0.5293333333333333,0.5293333333333333,zero_shot,0.996,classify_lora,0.8213333333333334,0.2608937108079508,0.0
alpaca_mmlu,reward_seeking,reward_seeking,0.49866666666666665,0.48,zero_shot,0.9186666666666666,classify_lora,0.8213333333333334,0.26454072589646765,0.020319303338171262
alpaca_mmlu,crt_1,crt_1,0.64,0.336,zero_shot,1.0,classify_lora,0.8213333333333334,0.2817090482714683,0.304
alpaca_mmlu,crt_2,crt_2,0.136,0.156,zero_shot,0.992,mms,0.8213333333333334,0.2806583269210747,-0.020161290322580634
alpaca_mmlu,crt_3,crt_3,0.676,0.248,zero_shot,1.0,classify_lora,0.8213333333333334,0.2803663661223862,0.42800000000000005
alpaca_mmlu,sycophancy_mimicry,quote_attribution,0.28,0.5,zero_shot,0.968,classify_lora,0.8213333333333334,0.25803505993953846,-0.22727272727272727
alpaca_mmlu,sycophancy_answer,arc_easy,0.356,0.012,zero_shot,0.9533333333333334,classify_lora,0.8213333333333334,0.2906599330822993,0.3608391608391608
alpaca_mmlu,sycophancy_feedback,code_is_correct,0.436,0.492,zero_shot,0.764,classify_lora,0.8213333333333334,0.19877504029046392,-0.07329842931937172
alpaca_chat,sycophancy_are_you_sure,arc_easy,0.34,1.0,zero_shot,1.0,few_shot,0.8426666666666667,0.15227445636728681,-0.6599999999999999
